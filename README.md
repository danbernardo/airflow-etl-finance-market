
# Pipeline ETL de AnÃ¡lise de Volatilidade Financeira

Pipeline de dados em produÃ§Ã£o que processa 750 mil registros de cotaÃ§Ãµes financeiras atravÃ©s de uma arquitetura Medallion (Bronze â†’ Silver â†’ Gold). Implementa orquestraÃ§Ã£o robusta com Apache Airflow, data warehouse normalizado em PostgreSQL e Star Schema otimizado para analytics.

---

## Objetivo do Projeto

- Ingerir e processar 750 mil registros de dados de mercado financeiro com eficiÃªncia
- Calcular indicadores de volatilidade semanal por ativo
- Estruturar dados em Star Schema para consultas analÃ­ticas
- Garantir rastreabilidade e reprodutibilidade do pipeline

---

## Dashboard Interativo

Os dados consolidados estÃ£o disponÃ­veis no PostgreSQL em tabelas normalizadas (Medallion Gold Layer) prontas para integraÃ§Ã£o com ferramentas de BI como Power BI, Looker ou Grafana.

**Indicadores disponÃ­veis**: volatilidade semanal, volume agregado, variaÃ§Ã£o diÃ¡ria, correlaÃ§Ãµes entre ativos.

---

## Tecnologias Utilizadas

| Componente | Tecnologia |
| --- | --- |
| OrquestraÃ§Ã£o | Apache Airflow (DAGs) |
| Data Warehouse | PostgreSQL (Star Schema) |
| Processamento | Python, Pandas |
| IngestÃ£o | COPY FROM nativo do PostgreSQL |
| VisualizaÃ§Ã£o | AnÃ¡lise exploratÃ³ria em Jupyter Notebooks |
| ContainerizaÃ§Ã£o | Docker Compose |
| Versionamento | GitHub |

---

## Fonte de Dados

Os dados utilizados no projeto foram extraÃ­dos de uma Ãºnica fonte:

| Fonte | ConteÃºdo | Quantidade |
| --- | --- | --- |
| **CSV Local** | CotaÃ§Ãµes diÃ¡rias de 750 mil registros (data, sÃ­mbolo, open, high, low, close, volume) | 1 arquivo |

---

## Arquitetura de Dados â€“ Medallion Architecture

O pipeline segue o modelo **Medallion Architecture** com trÃªs camadas de transformaÃ§Ã£o:

### Bronze Layer â€“ IngestÃ£o
Dados brutos carregados do CSV via `COPY FROM` PostgreSQL (750K registros em ~2 segundos). Tabela `staging` mantÃ©m dados originais sem transformaÃ§Ã£o.

### Silver Layer â€“ ValidaÃ§Ã£o
AplicaÃ§Ã£o de regras de qualidade de dados e criaÃ§Ã£o de dimensÃµes normalizadas (`dim_instrument`, `dim_tempo`).

### Gold Layer â€“ Analytics Ready
Tabelas fato (`fact_movimentacao_diaria`, `volatility_weekly`) estruturadas em Star Schema, otimizadas para queries analÃ­ticas.

---

-- Tabela Fato: Volatilidade Semanal
CREATE TABLE volatility_weekly AS
SELECT
    DATE_TRUNC('week', date)::date AS week_start,
    ticker,
    ROUND(STDDEV(variacao_diaria), 2) AS vol
FROM fact_movimentacao_diaria
GROUP BY DATE_TRUNC('week', date), ticker;
```

**CaracterÃ­sticas:**
- CÃ¡lculo automÃ¡tico de volatilidade (desvio padrÃ£o)
- AgregaÃ§Ãµes por semana e por ativo
- Indicadores de amplitude e variaÃ§Ã£o diÃ¡ria
- Pronto para relatÃ³rios executivos

---

## MÃ©tricas-Chave

### ï¿½ Volatilidade (Desvio PadrÃ£o da VariaÃ§Ã£o DiÃ¡ria)

Mede **o quanto o preÃ§o de um ativo varia**. AÃ§Ãµes com alta volatilidade tÃªm preÃ§os que oscilam muito, indicando maior potencial de ganho e maior risco de perda.

```sql
SELECT 
    ticker,
    STDDEV(variacao_diaria) AS volatilidade,
    AVG(variacao_diaria) AS retorno_medio,
    MAX(variacao_diaria) AS variacao_maxima,
    MIN(variacao_diaria) AS variacao_minima
FROM fact_movimentacao_diaria
GROUP BY ticker
ORDER BY volatilidade DESC;
```

**InterpretaÃ§Ã£o:**
- Valores **altos** indicam maior oscilaÃ§Ã£o de preÃ§os (risco elevado, potencial alto).
- Valores **baixos** indicam estabilidade (menor risco, retornos previsÃ­veis).

### Volume de OperaÃ§Ãµes

Indica o volume total negociado por ativo, sinalizando liquidez e interesse do mercado.

```sql
SELECT 
    ticker,
    AVG(volume) AS volume_medio,
    SUM(volume) AS volume_total
FROM fact_movimentacao_diaria
GROUP BY ticker
ORDER BY volume_total DESC;
```

---

## Modelagem Star Schema â€“ Tabelas DisponÃ­veis

### Tabelas Fato

| Tabela | DescriÃ§Ã£o | Granularidade |
| --- | --- | --- |
| `fact_movimentacao_diaria` | Registros de abertura, fechamento, mÃ¡xima, mÃ­nima, volume e variaÃ§Ã£o diÃ¡ria | DiÃ¡ria por ativo |
| `volatility_weekly` | Volatilidade agregada por semana | Semanal por ativo |

### Tabelas DimensÃ£o

- `dim_time`: Datas com hierarquia (ano, mÃªs, semana, dia).
- `dim_instrument`: Tickers dos ativos com suas metadatas.

> **ObservaÃ§Ã£o**: A normalizaÃ§Ã£o em dimensÃµes permite consultas otimizadas e reutilizaÃ§Ã£o em mÃºltiplos relatÃ³rios.

---

## Fluxo de TransformaÃ§Ã£o â€“ Pipeline Airflow

### ConcessÃµes CSV â†’ Staging â†’ Fact/Dimension Tables

| Etapa | ResponsÃ¡vel | DescriÃ§Ã£o |
| --- | --- | --- |
| **locate_csv** | Python Task | Valida existÃªncia do arquivo CSV (750k) |
| **load_staging** | PostgreSQL COPY | Carrega dados brutos na tabela staging |
| **data_quality** | SQL Check | Valida contagem de registros e nulos |
| **transform_dimensions** | SQL | Cria dimensÃµes (tempo, instrumento) |
| **transform_facts** | SQL | Cria tabela fato e calcula volatilidade |
| **report_top_volatility** | Python | Extrai e analisa ativos mais volÃ¡teis |
| **log_execution_summary** | Logging | Registra resumo da execuÃ§Ã£o |

**CaracterÃ­sticas da OrquestraÃ§Ã£o:**
- Retries automÃ¡ticos (3 tentativas com delay de 5 minutos)
- XCom para passagem de contexto entre tasks
- Schedule diÃ¡rio Ã s 7h da manhÃ£
- Fail-fast em validaÃ§Ãµes de qualidade

---

## Insights Destacados

- **Top 5 Ativos Mais VolÃ¡teis**: Identificados pela anÃ¡lise semanal e relatados automaticamente ao final de cada ciclo.
- **CorrelaÃ§Ãµes de Mercado**: AnÃ¡lise exploratÃ³ria detecta padrÃµes de movimento entre ativos.
- **Alertas de Risco**: Volatilidade acima de limiar dispara recomendaÃ§Ã£o para hedge e revisÃ£o de posiÃ§Ãµes.
- **Reprodutibilidade**: Todo cÃ¡lculo Ã© rastreÃ¡vel desde a ingestÃ£o atÃ© o relatÃ³rio final.

---

## Estrutura de Pastas do Projeto

```
ğŸ“¦ financial-market-analysis
â”‚
â”œâ”€â”€ dags
â”‚   â””â”€â”€ financial_pipeline.py             # DAG Airflow com orquestraÃ§Ã£o completa
â”‚
â”œâ”€â”€ sql
â”‚   â”œâ”€â”€ staging.sql                       # CriaÃ§Ã£o de tabela staging
â”‚   â”œâ”€â”€ dim_time.sql                      # DimensÃ£o de tempo
â”‚   â”œâ”€â”€ dim_instrument.sql                # DimensÃ£o de instrumento
â”‚   â”œâ”€â”€ fact_movimentacao.sql             # Fato movimentaÃ§Ã£o diÃ¡ria
â”‚   â”œâ”€â”€ volatility_weekly.sql             # AgregaÃ§Ã£o semanal de volatilidade
â”‚   â””â”€â”€ quality_checks.sql                # ValidaÃ§Ãµes de qualidade
â”‚
â”œâ”€â”€ analysis
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb        # AnÃ¡lise exploratÃ³ria com Pandas e visualizaÃ§Ãµes
â”‚   â””â”€â”€ volatility_report.ipynb           # RelatÃ³rio de volatilidade e recomendaÃ§Ãµes
â”‚
â”œâ”€â”€ scripts
â”‚   â””â”€â”€ [scripts auxiliares de ETL]
â”‚
â”œâ”€â”€ docker-compose.yml                    # OrquestraÃ§Ã£o de Airflow + PostgreSQL
â”œâ”€â”€ Dockerfile                            # Imagem customizada do Airflow
â”œâ”€â”€ start_services.bat                    # Script para iniciar stack
â”œâ”€â”€ financial_market_750k.csv             # Dataset de entrada (750k registros)
â””â”€â”€ README.md                             # DocumentaÃ§Ã£o do projeto
```

---

## Como Reproduzir

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Python 3.9+
- PostgreSQL 12+ (serÃ¡ iniciado via Docker)
- Apache Airflow 2.0+ (serÃ¡ iniciado via Docker)

### Passos de ExecuÃ§Ã£o

1. **Clone ou navegue atÃ© o diretÃ³rio do projeto:**
   ```bash
   cd c:\Users\danie\Downloads\project
   ```

2. **Configure o arquivo `.env`** (variÃ¡veis de ambiente seguras):
   
   Copie o arquivo de exemplo:
   ```bash
   copy .env.example .env
   ```
   
   Edite o arquivo `.env` com suas credenciais seguras:
   ```env
   # Gere novas chaves Fernet e SECRET_KEY para produÃ§Ã£o:
   # Fernet Key: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
   
   POSTGRES_PASSWORD=sua_senha_segura_aqui
   AIRFLOW_WWW_USER_PASSWORD=sua_senha_webui_aqui
   AIRFLOW_FERNET_KEY=sua_chave_fernet_aqui
   AIRFLOW_SECRET_KEY=sua_chave_secreta_aqui
   AIRFLOW_CONN_POSTGRES_DW=postgresql://airflow:sua_senha_segura_aqui@postgres:5432/financial_dw
   AIRFLOW_DATABASE_SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:sua_senha_segura_aqui@postgres/airflow
   ```
   
   **IMPORTANTE**: O arquivo `.env` estÃ¡ no `.gitignore` e nunca serÃ¡ commitado no repositÃ³rio.

3. **Inicie os serviÃ§os (Airflow + PostgreSQL):**
   ```bash
   docker compose down -v
   docker compose up -d
   ```

4. **Aguarde 30-60 segundos** para inicializaÃ§Ã£o completa.

5. **Acesse o Airflow UI:**
   - URL: `http://localhost:58080` (ou `http://<WSL_IP>:58080` se rodando em WSL)
   - UsuÃ¡rio: (conforme definido em `AIRFLOW_WWW_USER_USERNAME` no `.env`)
   - Senha: (conforme definido em `AIRFLOW_WWW_USER_PASSWORD` no `.env`)

   **Exemplo de login (baseado em `.env`):**
   ```
   UsuÃ¡rio: admin1
   Senha: sua_senha_webui_aqui
   ```

6. **Ative a DAG `financial_volatility_pipeline`** no Airflow UI.

7. **Trigger manual ou aguarde o schedule** (diariamente Ã s 7h).

8. **Consulte os resultados** no PostgreSQL ou nos notebooks Jupyter:
   ```bash
   jupyter notebook analysis/exploratory_analysis.ipynb
   ```

### ValidaÃ§Ã£o

ApÃ³s a execuÃ§Ã£o, consulte as tabelas Gold:
```sql
SELECT * FROM volatility_weekly ORDER BY week_start DESC LIMIT 10;
SELECT * FROM fact_movimentacao_diaria LIMIT 100;
```

---

## Login no Airflow UI â€“ Fluxo Passo-a-Passo

### 1. Abra o navegador e acesse a URL

```
http://localhost:58080
```

> Se estiver rodando em WSL sem Docker Desktop, use o IP da mÃ¡quina WSL:
> ```
> http://<seu_wsl_ip>:58080
> ```
> Para encontrar o IP: execute `hostname -I` dentro da WSL.

### 2. VocÃª verÃ¡ a tela de login do Airflow

A pÃ¡gina exibirÃ¡ dois campos:
- **Username** (ou Email)
- **Password**

### 3. Digite suas credenciais do `.env`

Use as variÃ¡veis que vocÃª configurou:

```
Username: admin1                          (valor de AIRFLOW_WWW_USER_USERNAME)
Password: sua_senha_webui_aqui            (valor de AIRFLOW_WWW_USER_PASSWORD)
```

**Exemplo completo baseado no `.env`:**
```env
AIRFLOW_WWW_USER_USERNAME=admin1
AIRFLOW_WWW_USER_PASSWORD=MinhaSenh@Segur@2025
```

EntÃ£o o login seria:
```
Username: admin1
Password: MinhaSenh@Segur@2025
```

### 4. Clique em "Sign In"

ApÃ³s autenticar com sucesso, vocÃª acessarÃ¡ o dashboard principal do Airflow onde pode:
- Visualizar DAGs
- Monitorar execuÃ§Ãµes
- Acionar pipelines manualmente
- Consultar logs

---

## Limpeza e ManutenÃ§Ã£o

### Logs (Airflow)

Os logs sÃ£o armazenados em `logs/` e registram **todas as execuÃ§Ãµes da DAG**. SÃ£o temporÃ¡rios e podem crescer bastante.

**Quando limpar:**
- Antes de compartilhar o repositÃ³rio (contÃªm timestamps e IPs internos)
- Para liberar espaÃ§o em disco
- Quando quer "resetar" o histÃ³rico de execuÃ§Ãµes

**Como limpar:**

```bash
# Limpar todos os logs (mantÃ©m estrutura de pastas)
Remove-Item -Path "logs/*" -Recurse -Force

# Ou ao parar os containers
docker compose down -v
```
=======
# airflow-etl-finance-market
Pipeline ETL Mercado Financeiro com Airflow, Pandas e PostgreSQL â€” Exemplo didÃ¡tico.
>>>>>>> be23e7ccd61027f8f4b51f2438789b68f08a856b
